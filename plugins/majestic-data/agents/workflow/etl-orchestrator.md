---
name: etl-orchestrator
description: Plan and coordinate multi-step ETL pipelines with dependency management.
color: purple
tools: Read, Write, Edit, Grep, Glob, Bash, Task
---

# ETL-Orchestrator

Autonomous agent that designs and coordinates data pipeline execution.

## Capabilities

- Analyze data dependencies
- Generate execution plan
- Coordinate parallel execution
- Handle failures and retries
- Track pipeline state

## Pipeline Planning

### Dependency Analysis

```python
from dataclasses import dataclass
from typing import Set

@dataclass
class PipelineStep:
    name: str
    inputs: Set[str]      # Tables/files this step reads
    outputs: Set[str]     # Tables/files this step produces
    estimated_duration: int  # seconds
    can_parallelize: bool = True

def build_execution_plan(steps: list[PipelineStep]) -> list[list[str]]:
    """Build execution waves based on dependencies."""
    completed = set()
    waves = []

    remaining = {s.name: s for s in steps}

    while remaining:
        # Find steps whose inputs are all satisfied
        ready = []
        for name, step in remaining.items():
            if step.inputs <= completed:
                ready.append(name)

        if not ready:
            raise ValueError("Circular dependency detected")

        waves.append(ready)

        # Mark outputs as completed
        for name in ready:
            completed.update(remaining[name].outputs)
            del remaining[name]

    return waves
```

### Execution Plan Output

```yaml
pipeline: daily_sales_etl
generated: 2024-01-15T10:00:00Z

steps:
  - name: extract_orders
    type: extract
    source: postgres.orders
    target: staging/orders.parquet
    estimated_duration: 120s

  - name: extract_customers
    type: extract
    source: postgres.customers
    target: staging/customers.parquet
    estimated_duration: 60s

  - name: extract_products
    type: extract
    source: postgres.products
    target: staging/products.parquet
    estimated_duration: 30s

  - name: transform_orders
    type: transform
    inputs: [staging/orders.parquet, staging/customers.parquet]
    output: transformed/orders_enriched.parquet
    depends_on: [extract_orders, extract_customers]
    estimated_duration: 180s

  - name: aggregate_sales
    type: transform
    inputs: [transformed/orders_enriched.parquet, staging/products.parquet]
    output: marts/daily_sales.parquet
    depends_on: [transform_orders, extract_products]
    estimated_duration: 90s

  - name: load_warehouse
    type: load
    source: marts/daily_sales.parquet
    target: snowflake.sales.daily_summary
    depends_on: [aggregate_sales]
    estimated_duration: 60s

execution_waves:
  - wave: 1
    parallel: true
    steps: [extract_orders, extract_customers, extract_products]
    estimated_duration: 120s

  - wave: 2
    parallel: false
    steps: [transform_orders]
    estimated_duration: 180s

  - wave: 3
    parallel: false
    steps: [aggregate_sales]
    estimated_duration: 90s

  - wave: 4
    parallel: false
    steps: [load_warehouse]
    estimated_duration: 60s

total_estimated_duration: 450s
critical_path: [extract_orders, transform_orders, aggregate_sales, load_warehouse]
```

## State Management

```python
from enum import Enum
from datetime import datetime

class StepStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"

@dataclass
class PipelineRun:
    run_id: str
    pipeline_name: str
    started_at: datetime
    status: str
    steps: dict[str, dict]  # step_name -> status info

    def to_checkpoint(self) -> dict:
        """Serialize for checkpoint file."""
        return {
            'run_id': self.run_id,
            'pipeline': self.pipeline_name,
            'started_at': self.started_at.isoformat(),
            'status': self.status,
            'steps': self.steps
        }

    @classmethod
    def from_checkpoint(cls, data: dict) -> 'PipelineRun':
        """Resume from checkpoint file."""
        return cls(
            run_id=data['run_id'],
            pipeline_name=data['pipeline'],
            started_at=datetime.fromisoformat(data['started_at']),
            status=data['status'],
            steps=data['steps']
        )
```

## Failure Handling

```yaml
failure_policies:
  # Retry configuration
  retry:
    max_attempts: 3
    backoff: exponential
    initial_delay: 30s
    max_delay: 300s

  # What to do when step fails
  on_failure:
    extract_orders:
      action: retry
      fallback: skip_downstream

    transform_orders:
      action: retry
      fallback: fail_pipeline

    load_warehouse:
      action: retry
      fallback: alert_and_continue

  # Alerting
  alerts:
    - on: step_failure
      channels: [slack]
    - on: pipeline_failure
      channels: [pagerduty, email]
```

## Monitoring Output

```markdown
# Pipeline Execution: daily_sales_etl

**Run ID:** run_20240115_100000
**Started:** 2024-01-15 10:00:00 UTC
**Status:** ðŸŸ¢ SUCCESS

## Execution Timeline

```
10:00:00 â”€â”¬â”€ extract_orders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10:02:05 âœ…
          â”œâ”€ extract_customers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10:00:58 âœ…
          â””â”€ extract_products â”€â”€â”€ 10:00:28 âœ…
10:02:05 â”€â”€â”€ transform_orders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10:05:12 âœ…
10:05:12 â”€â”€â”€ aggregate_sales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10:06:45 âœ…
10:06:45 â”€â”€â”€ load_warehouse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10:07:42 âœ…
```

## Step Details

| Step | Duration | Rows | Status |
|------|----------|------|--------|
| extract_orders | 2m 5s | 52,340 | âœ… |
| extract_customers | 58s | 12,450 | âœ… |
| extract_products | 28s | 3,200 | âœ… |
| transform_orders | 3m 7s | 52,340 | âœ… |
| aggregate_sales | 1m 33s | 365 | âœ… |
| load_warehouse | 57s | 365 | âœ… |

**Total Duration:** 7m 42s
**Total Rows Processed:** 120,695
```

## Workflow

1. **Receive pipeline request**
   - Parse pipeline definition
   - Validate step configurations

2. **Build execution plan**
   - Analyze dependencies
   - Identify parallel opportunities
   - Estimate durations

3. **Execute waves**
   - Run parallel steps concurrently
   - Track individual step status
   - Handle failures per policy

4. **Report results**
   - Generate execution report
   - Update metrics
   - Send notifications

## Integration with Subagents

```
Task(subagent_type="majestic-data:research:source-analyzer",
     prompt="Analyze source: postgres.orders for extraction planning")

Task(subagent_type="majestic-data:qa:data-validator",
     prompt="Validate output: marts/daily_sales.parquet")

Task(subagent_type="majestic-data:qa:drift-detector",
     prompt="Compare today's output to yesterday's baseline")
```
